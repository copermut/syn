---
layout: post
---
> #### _Everything Is Connected_

> Not only information theory and machine learning, which are two sides of a single coin; but also communication, data compression, evolution, sex, satellites, discdrives, solitons, thermodynamics - pretty much any topics you care to mention.

— Sir David J. C. Mackay FRS ([R.I.P.](http://biochemistri.es/in-memoriam-david-mackay-frs))

- The first time I saw the word "connectionist" was on [a mailing list](http://mailman.srv.cs.cmu.edu/pipermail/connectionists/1992-October.txt) following the passing of Professor David J. C. Mackay, trying to verify an intuition I'd had about the poetics of the above quote from his blog in regards to his textbook, ITILA (_Information Theory, Inference, and Learning Algorithms_), _"whose goals include bringing out these connections"_.  
  
  From the mailing list, around the time of the "spectacular takeover" of the NIPS conference in the 1990s [described here](https://books.google.co.uk/books?id=pjRkCQAAQBAJ&pg=PT161&lpg=PT161&source=bl&f=false#v=onepage&q&f=false):

  - Kenneth D Miller and David J C MacKay (1994) [The Role of Constraints in Hebbian Learning](http://www.neurotheory.columbia.edu/Ken/pubs/miller_mackay94.pdf)

- Fernando Pereira (_Google_), [Four connected thoughts on language, deep learning, and memory](https://plus.google.com/+FernandoPereira/posts/PCcctKkSmpj):  

  >  1.  I. J. Good wrote a neat, subtle argument that Markov separation between past and future in text requires growing capacity as the corpus grows (in Encyclopaedia of linguistics, information, and control, 1969, which I don't have at hand but never forgot).  

  >  2. Tali Tishby and colleagues explained that information bottlenecks between past and future are only interesting if their capacity does not grow too fast with the span being accounted for.  

  >    - Creutzig &a., [Past-future information bottleneck in dynamical systems](http://journals.aps.org/pre/abstract/10.1103/PhysRevE.79.041925).  

  >  3. Long-term memory in a noisy environment seems to require "symbols" (stable substates).
  >  4. End-to-end learning of linguistic behavior will thus require a slowly-enough growing symbolic memory — although the "symbols" would be emergent, not externally given.﻿  
  
   - [Response](https://twitter.com/egrefen/status/647061478940540928) by Edward Grefenstette (_Google DeepMind_):  

    > Agree on points 3/4. __No more symbolic vs. connectionist distinction.__ Learn symbolic reasoning over continuous representations.

    - Also seen in [Five lessons from AlphaGo's historic victory](https://www.technologyreview.com/s/601072/five-lessons-from-alphagos-historic-victory/), _MIT Technology Review_ (March 18 2016)

- Shubhendu Trivedi (ML, AI, graphs, combinatorics):

  - [Tweet](https://twitter.com/_onionesque/status/156276103886024704): "Excellent article on connectionist and "hypercomputation" ideas that Alan Turing had but were forgotten" - [Alan Turing's Forgotten Ideas in Computer Science](http://www.nature.com/scientificamerican/journal/v280/n4/pdf/scientificamerican0499-98.pdf), _Sci Am_ (1999) <sup>`TODO:read`</sup>
  - [Tweet](https://twitter.com/_onionesque/status/229860946527543296): "All the 'connectionist' work aside, I love this '87 paper by Geoff Hinton on showing how "Baldwinian Evolution" matters" - [How Learning Can Guide Evolution](http://www.icts.res.in/media/uploads/Talk/Document/How%20Learning%20Can%20Guide%20Evolution%20-1987.pdf) <sup>`TODO:read`</sup>

- Beau Cronin (_21_, AI, VR):

  - [Tweet](https://twitter.com/beaucronin/status/429692473154084864): "previous from long discussion of the meaning of "brain-like computing", among other topics, on the connectionist list"
    - [Tweet](https://twitter.com/beaucronin/status/429740538380562432): "fascinating discussion/flamewar on connectionism" <sup>`TODO:read`</sup>

- Xiang Zhang (NYU PhD student, ML, Torch):

  - [Tweet](https://twitter.com/G_Auss/status/690868158480138241): "Instead of learning a connectionist model to do arithmetic operations, how about evolving a machine to make... http://fb.me/CKttiRst" - [Evolve to Sum](http://xzh.me/posts/evolvetosum/) <sup>`TODO:read`</sup>
    - NB: Facebook posts containing links are redirected to the link, text cut off can't be seen from outside 'walled garden'. Link points to Xiang's blog:
    - reminiscent of the _FizzBuzz in TensorFlow_ [joke] example (in fact would guess it foresaw it)

- Patrick Muncaster (AI)
  - "[Connectionism: philosophy meets cognitive and neurosciences](http://www.technology.org/2014/08/21/connectionism-philosophy-meets-cognitive-neurosciences/) - for rumination" <sup>`TODO:read`</sup> 

- "Connectionist temporal modelling" and "connectionist temporal classification"
  - 31st July 2016: [Connectionist Temporal Modeling for Weakly Supervised Action Labeling](https://arxiv.org/abs/1607.08584) (via: [deeplearning4j tweet](https://twitter.com/deeplearning4j/status/759630101608747008)) <sup>`TODO:read`</sup>
  - Google's September 24th 2015 announcement of [Google voice search: faster and more accurate](https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html) <sup>`TODO:read`</sup> noted using "Connectionist Temporal Classification (CTC)... a special extension of RNNs... more accurate, especially in noisy environments, and blazing fast", citing a 2006 ICML paper [Connectionist Temporal Classification: Labelling Unsegmented
Sequence Data with Recurrent Neural Networks](ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf) <sup>`TODO:read`</sup>